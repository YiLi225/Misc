{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d830a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "How to choose learning rate with Neptune\n",
    "'''\n",
    "###### Create Neptune project \n",
    "## update: pip install neptune-client==0.10.7\n",
    "import neptune\n",
    "import os\n",
    "\n",
    "# Connect your script to Neptune\n",
    "project = neptune.init(api_token=os.getenv('NEPTUNE_API_TOKEN'),\n",
    "                       project_qualified_name='YourUserName/YourProjectName') \n",
    "\n",
    "\n",
    "### from sklearn.datasets import load_iris \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.callbacks as callbacks\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, concatenate, MaxPooling2D, Conv2D\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler \n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow import random_normal_initializer\n",
    "from numpy import argmax\n",
    "from collections import Counter\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import TensorBoard\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "#### Load data for the image classifier model\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test_full, y_test_full) = fashion_mnist.load_data()\n",
    "\n",
    "reset_random_seeds()\n",
    "trainIdx = random.sample(range(60000), 20000)\n",
    "\n",
    "x_train, y_train = X_train_full[trainIdx]/255.0, y_train_full[trainIdx]\n",
    "x_test, y_test = X_test_full/255.0, y_test_full\n",
    "    \n",
    "#### Save learning rate during the training    \n",
    "def get_lr_metric(optimizer):\n",
    "    def lr(y_true, y_pred):\n",
    "        curLR = optimizer._decayed_lr(tf.float32)\n",
    "        return curLR # use ._decayed_lr method instead of .lr\n",
    "    return lr\n",
    "\n",
    "### Function to plot the learning rate \n",
    "def plotLR(history):\n",
    "    learning_rate = history.history['lr']\n",
    "    epochs = range(1, len(learning_rate) + 1)\n",
    "    fig = plt.figure(figsize=(12, 7))\n",
    "    plt.plot(epochs, learning_rate)\n",
    "    plt.title('Learning rate')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Learning rate')\n",
    "    return(fig)\n",
    "\n",
    "#### Define the Neural Network model\n",
    "def runModel():   \n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=[28, 28])) \n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "model = runModel()\n",
    "model.summary()\n",
    "\n",
    "\n",
    "### Learning Rate Schedulers ###\n",
    "import math\n",
    "### in the console printout, we can see the learning rate difference  \n",
    "initial_learning_rate = 0.01\n",
    "epochs = 100\n",
    "decay = initial_learning_rate / epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511814de",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_LR_SCHEDULER = 'constant'\n",
    "# CURRENT_LR_SCHEDULER = 'time-based'\n",
    "# CURRENT_LR_SCHEDULER, POLY_POWER = 'polynomial', 'linear'\n",
    "\n",
    "\n",
    "### Functions to plot the train history \n",
    "def plotPerformance(history, CURRENT_LR_SCHEDULER=CURRENT_LR_SCHEDULER):\n",
    "    #### Loss\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    fig = plt.subplot(1, 2, 1) # row 1, col 2 index 1\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.legend(['Train Loss', 'Test Loss'])\n",
    "    plt.title(f'Loss Curves ({CURRENT_LR_SCHEDULER})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss on the Validation Set')\n",
    "    \n",
    "    #### Accuracy \n",
    "    fig = plt.subplot(1, 2, 2) # row 1, col 2 index 1\n",
    "\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.legend(['Train Accuracy', 'Test Accuracy'])\n",
    "    plt.title(f'Accuracy Curves ({CURRENT_LR_SCHEDULER})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy on the Validation Set')\n",
    "    return fig\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "if CURRENT_LR_SCHEDULER == 'constant':\n",
    "    # Create an experiment and log the model \n",
    "    npt_exp = project.create_experiment(name='ConstantLR', \n",
    "                                        description='constant-lr', \n",
    "                                        tags=['LearingRate', 'constant', 'baseline', 'neptune'])       \n",
    "        \n",
    "    ### Baseline model: constant learning rate \n",
    "    initial_learning_rate = 0.01\n",
    "    epochs = 100\n",
    "    sgd = keras.optimizers.SGD(learning_rate=initial_learning_rate)\n",
    "    lr_metric = get_lr_metric(sgd)\n",
    "    \n",
    "    model.compile(optimizer = sgd,\n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy', lr_metric]) \n",
    "    \n",
    "    reset_random_seeds()\n",
    "    \n",
    "    trainHistory_constantLR = model.fit(\n",
    "        x_train, y_train, \n",
    "        epochs=epochs,\n",
    "        validation_data=(x_test, y_test),\n",
    "        batch_size=64\n",
    "    )\n",
    "    \n",
    "    ### Plot learning rate over time \n",
    "    npt_exp.log_image('Learning Rate Change (Constant)', plotLR(trainHistory_constantLR))\n",
    "    \n",
    "    ### Plot the training history \n",
    "    npt_exp.log_image('Training Performance Curves (Constant)', plotPerformance(trainHistory_constantLR).get_figure())\n",
    "    \n",
    "elif CURRENT_LR_SCHEDULER == 'time-based':\n",
    "    ## initial learning rate set to a larger number \n",
    "    initial_learning_rate = 0.5 \n",
    "    epochs = 100\n",
    "    decay = initial_learning_rate/epochs   \n",
    "\n",
    "    # Create an experiment and log the model \n",
    "    npt_exp = project.create_experiment(name='TimeBasedLRDecay', \n",
    "                                        description='time-based-lr-decay', \n",
    "                                        tags=['LearningRate', 'timebased', 'decay', 'neptune'])       \n",
    "\n",
    "    def lr_time_based_decay(epoch, lr):\n",
    "        return lr * 1 / (1 + decay * epoch)\n",
    "    \n",
    "    model = runModel()\n",
    "    model.summary()\n",
    "    \n",
    "    sgd = keras.optimizers.SGD(learning_rate=initial_learning_rate) \n",
    "    model.compile(\n",
    "                  optimizer = sgd,\n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy']) \n",
    "    \n",
    "    reset_random_seeds()\n",
    "    \n",
    "    trainHistory_timeBasedDecay = model.fit(\n",
    "        x_train, y_train, \n",
    "        epochs=epochs, \n",
    "        batch_size=64,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[LearningRateScheduler(lr_time_based_decay, verbose=1)])    \n",
    "    \n",
    "    ### Plot learning rate over time \n",
    "    npt_exp.log_image('Learning Rate Change (Time-Based Decay)', plotLR(trainHistory_timeBasedDecay))\n",
    "    ### Plot the training history \n",
    "    npt_exp.log_image('Training Performance Curves (Time-Based Decay)', plotPerformance(trainHistory_timeBasedDecay).get_figure())\n",
    "\n",
    "elif CURRENT_LR_SCHEDULER == 'polynomial':\n",
    "    initial_learning_rate = 0.5 \n",
    "    epochs = 100\n",
    "    decay = initial_learning_rate/epochs   \n",
    "    \n",
    "    ## Defined as a class to save parameters as attributes\n",
    "    class lr_polynomial_decay:\n",
    "    \tdef __init__(self, epochs=100, initial_learning_rate=0.01, power=1.0):\n",
    "    \t\t# store the maximum number of epochs, base learning rate, and power of the polynomial\n",
    "    \t\tself.epochs = epochs\n",
    "    \t\tself.initial_learning_rate = initial_learning_rate\n",
    "    \t\tself.power = power\n",
    "            \n",
    "    \tdef __call__(self, epoch):\n",
    "    \t\t# compute the new learning rate based on polynomial decay\n",
    "    \t\tdecay = (1 - (epoch / float(self.epochs))) ** self.power\n",
    "    \t\tupdated_eta = self.initial_learning_rate * decay\n",
    "    \t\t# return the new learning rate\n",
    "    \t\treturn float(updated_eta)\n",
    "        \n",
    "    def plot_Neptune(history, decayTitle):\n",
    "        ### Plot learning rate over time \n",
    "        npt_exp.log_image(f'Learning Rate Change ({decayTitle})', plotLR(history))\n",
    "        ### Plot the training history \n",
    "        npt_exp.log_image(f'Training Performance Curves ({decayTitle})', plotPerformance(history).get_figure())\n",
    "    \n",
    "    # Create an experiment and log the model \n",
    "    npt_exp = project.create_experiment(name=f'{POLY_POWER}LRDecay', \n",
    "                                        description=f'{POLY_POWER}-lr-decay', \n",
    "                                        tags=['LearningRate', POLY_POWER, 'decay', 'neptune'])   \n",
    "       \n",
    "    if POLY_POWER == 'linear':\n",
    "        curPower = 1.0\n",
    "   \n",
    "    curScheduler = lr_polynomial_decay(epochs=epochs, initial_learning_rate=initial_learning_rate, power=curPower)\n",
    "    \n",
    "    model = runModel()\n",
    "    model.summary()\n",
    "    \n",
    "    sgd = keras.optimizers.SGD(learning_rate=initial_learning_rate) \n",
    "    model.compile(\n",
    "                  optimizer = sgd,\n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy']) \n",
    "    \n",
    "    reset_random_seeds()\n",
    "    \n",
    "    trainHistory_polyDecay = model.fit(\n",
    "        x_train, y_train, \n",
    "        epochs=epochs, \n",
    "        batch_size=64,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[LearningRateScheduler(curScheduler, verbose=1)]) \n",
    "    \n",
    "    if POLY_POWER == 'linear':\n",
    "        trainHistory_linearDecay = trainHistory_polyDecay\n",
    "        plot_Neptune(history=trainHistory_linearDecay, decayTitle='Linear Decay')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
